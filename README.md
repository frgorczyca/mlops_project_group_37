# text_detect

Text_detect is an exam project for the DTU course 02476 MLOps.
The goal is to develop a machine learning pipeline and use a machine learning model to detect whether a text is generated by an AI or written by real people.
The data we have used is the kaggle dataset "DAIGT Proper Train Dataset" (https://www.kaggle.com/datasets/thedrcat/daigt-proper-train-dataset/data?select=train_drcat_04.csv) and the base-model used for the classification is the LLM transformer RoBERTa with pretrained weights (https://huggingface.co/FacebookAI/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal). We will primarily be using Pytorch as our framework.

## How to use

To-be-updated


## Artifact staging
Works as follows:
1. A model is trained using invoke train, and is saved as an artifact on Weights and Biases under the given project and artifact name. The metrics of that artifacts run are also saved.
2. invoke evaluate.py can be called with the artifacts name and version (format: artifact_name:version) to evaluate it on the test dataset.
3. When invoke stage-best-model is run, all versions of the artifact name are compared on the given metric, and the best performing version is staged to the organizations (not teams) given Weights and Biases artifact registry collection, where it gets the aliases "best" and "staging"
4. An automation event on the artifact registry collection called "staged_model" is triggered when the "staging" alias is added to a linked model, triggering a GitHub action workflow that listens to the /dispatches endpoint. It recieves a JSON payload that contains information about the model that was staged, such as its name etc.
5. Upon "stage_model" event being triggered, the stage_model.yaml workflow is executed, which does the following:
   1. Checks the event type is correct and outputs the model name from the payload as an environment variable
   2. Tests the staged model on tests/performancetests/test_model.py. The repo needs to have certain environent variables defined as secrets to work
   3. If the staged model passes the test(s), it gets the alias "production" added, and the workflow is complete

WANDB_API_KEY
WANDB_ENTITY
WANDB_TEAM
WANDB_PROJECT


## Project structure

The directory structure of the project looks like this:
```txt
├── .github/                  # Github actions and dependabot
│   ├── dependabot.yaml
│   └── workflows/
│       └── tests.yaml
├── configs/                  # Configuration files
├── data/                     # Data directory
│   ├── processed
│   └── raw
├── dockerfiles/              # Dockerfiles
│   ├── api.Dockerfile
│   └── train.Dockerfile
├── docs/                     # Documentation
│   ├── mkdocs.yml
│   └── source/
│       └── index.md
├── models/                   # Trained models
├── notebooks/                # Jupyter notebooks
├── reports/                  # Reports
│   └── figures/
├── src/                      # Source code
│   ├── project_name/
│   │   ├── __init__.py
│   │   ├── api.py
│   │   ├── data.py
│   │   ├── evaluate.py
│   │   ├── models.py
│   │   ├── train.py
│   │   └── visualize.py
└── tests/                    # Tests
│   ├── __init__.py
│   ├── test_api.py
│   ├── test_data.py
│   └── test_model.py
├── .gitignore
├── .pre-commit-config.yaml
├── downloadKaggleDataset.sh  # Bash script to download datasets
├── LICENSE
├── pyproject.toml            # Python project file
├── README.md                 # Project README
├── requirements.txt          # Project requirements
├── requirements_dev.txt      # Development requirements
└── tasks.py                  # Project tasks
```

## Contributers

Artur Adam Habuda s233190
Eline Siegumfeldt s183540
Franciszek Marek Gorczyca s233664
Max-Peter Schrøder s214238
